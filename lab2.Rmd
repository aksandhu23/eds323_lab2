---
title: "Lab 2"
author: "Amritpal Sandhu"
date: "2023-01-18"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library("tidymodels")
library("tidyverse")
library("dplyr")
library("janitor")
library("corrplot")
dat <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/pumpkin-data.csv")
```

Case Study: The Pumpkin Market The data you just loaded includes 1757 lines of data about the market for pumpkins, sorted into groupings by city. This is raw data extracted from the Specialty Crops Terminal Markets Standard Reports distributed by the United States Department of Agriculture.

```{r data}
glimpse(dat)
```

```{r}
# Clean names to the snake_case convention

pumpkins <- dat %>% clean_names(case = "snake")

# Return column names

pumpkins %>% names()
```

## Select desired columns

```{r}
pumpkins <- pumpkins %>% select(variety, city_name, package, low_price, high_price, date)

## Print data set

pumpkins %>% slice_head(n = 5)
```

```{r}
## Load lubridate

library(lubridate)

# Extract the month and day from the dates and add as new columns
pumpkins <- pumpkins %>%
  mutate(date = mdy(date),  
         day = yday(date),
         month = month(date))
pumpkins %>% 
  select(-day)

## View the first few rows

pumpkins %>% slice_head(n = 7)
```

There are two column dealing with price, high and low. Let's combine them into a single average price column.

```{r}
# Create a new column price
pumpkins <- pumpkins %>% 
  mutate(price = (low_price+ high_price)/2)
```

Let's take a look at pumpkins sales throughout the year.

*Question 1:* Create a scatter plot using price on the y-axis and day on the x-axis.

```{r, echo=TRUE}
ggplot(data = pumpkins, aes(x = day, y = price)) + geom_point(color = "blue", 
             alpha = .2, 
             size = 3) +
  labs(x = "Day", y = "Price", title = "Pumpkin Sales Throughout the Year")
```

Now, before we go any further, let's take another look at the data. Notice anything odd?

That's right: pumpkins are sold in many different configurations. Some are sold in 1 1/9 bushel measures, and some in 1/2 bushel measures, some per pumpkin, some per pound, and some in big boxes with varying widths.

Let's verify this:

```{r}
# Verify the distinct observations in Package column
pumpkins %>% 
  distinct(package)
```

Pumpkins seem to be very hard to weigh consistently, so let's filter them by selecting only pumpkins with the string bushel in the package column and put this in a new data frame "new_pumpkins".

*Question 2* In the first section of the chunk below, use a combination of dplyr::filter() and stringr::str_detect() to achieve what we want.

```{r, echo=TRUE}
# Retain only pumpkins with "bushel" in the package column
new_pumpkins <- pumpkins %>% filter(str_detect(package, "bushel"))

# Get the dimensions of the new data
dim(new_pumpkins)

# View a few rows of the new data
new_pumpkins %>% 
  slice_head(n = 10)
```

You can see that we have narrowed down to 415 rows of data containing pumpkins by the bushel.

But wait! There's one more thing to do

Did you notice that the bushel amount varies per row? You need to normalize the pricing so that you show the pricing per bushel, not per 1 1/9 or 1/2 bushel. Time to do some math to standardize it.

We'll use the function case_when() to mutate the Price column depending on some conditions. case_when allows you to vectorise multiple if_else()statements.

```{r}
# Convert the price if the Package contains fractional bushel values
new_pumpkins <- new_pumpkins %>% 
  mutate(price = case_when(
    str_detect(package, "1 1/9") ~ price/(1.1),
    str_detect(package, "1/2") ~ price*2,
    TRUE ~ price))

# View the first few rows of the data
new_pumpkins %>% 
  slice_head(n = 30)
```

## Data Visualization

```{r}
# Set theme
theme_set(theme_light())

# Make a scatter plot of month and price
new_pumpkins %>% 
  ggplot(mapping = aes(x = day, y = price)) +
  geom_point(size = 1.6)
```

*Question 3:* Is this a useful plot ðŸ¤·? Does anything about it surprise you?

**The plot does provide information about pumpkin prices, however, this plot is not very useful as it is difficult to interpret and there is no discernible pattern. Also, there are too many points on the graph that make it look clunky. It surprising how the prices fluctuate drastically between days 275 to 315.**

How do we make it useful? To get charts to display useful data, you usually need to group the data somehow.

**We can make it useful by grouping the days into weeks or months and averaging the pumpkin prices.**

*Question 4:* Within new_pumpkins, group the pumpkins into groups based on the month column and then find the mean price for each month (in the next chunk).

Hint: use dplyr::group_by() %\>% summarize()

```{r, echo=TRUE}
# Find the average price of pumpkins per month
new_pumpkins %>% 
  group_by(month) %>% 
  summarize(price_mean = mean(price), na.rm = TRUE)
```

*Question 5:* Now do that again, but continue on and plot the results with a bar plot

```{r, echo=TRUE}
# Find the average price of pumpkins per month then plot a bar chart
pumpkins %>% 
  group_by(month) %>% 
  summarize(price_mean = mean(price), na.rm = TRUE) %>% 
  ggplot(aes(x = month, y = price_mean)) + geom_col()
```

#Preprocessing data for modelling using recipes

What if we wanted to predict the price of a pumpkin based on the city or package columns which are of type character? How could we find the correlation between, say, package and price?

Machine learning models work best with numeric features rather than text values, so you generally need to convert categorical features into numeric representations.

This means that we have to find a way to reformat our predictors to make them easier for a model to use effectively, a process known as **feature engineering**.

Different models have different preprocessing requirements. For instance, least squares requires encoding categorical variables such as month, variety and city_name. This simply involves translating a column with categorical values into one or more numeric columns that take the place of the original.

Now let's introduce another useful Tidymodels package: recipes - which will help you preprocess data before training your mode. A recipe is an object that defines what steps should be applied to a data set in order to get it ready for modelling.

Now, let's create a recipe that prepares our data for modelling by substituting a unique integer for all the observations in the predictor columns:

```{r}
# Specify a recipe
pumpkins_recipe <- recipe(price ~ ., data = new_pumpkins) %>% 
  step_integer(all_predictors(), zero_based = TRUE)


# Print out the recipe
pumpkins_recipe
```

OK, we created our first recipe that specifies an outcome (price) and its corresponding predictors and that all the predictor columns should be encoded into a set of integers. Let's quickly break it down:

The call to recipe() with a formula tells the recipe the roles of the variables using new_pumpkins data as the reference. For instance the price column has been assigned an outcome role while the rest of the columns have been assigned a predictor role.

step_integer(all_predictors(), zero_based = TRUE) specifies that all the predictors should be converted into a set of integers with the numbering starting at 0.

How can we confirm that the recipe is doing what we intend? Once your recipe is defined, you can estimate the parameters required to preprocess the data, and then extract the processed data. You don't typically need to do this when you use Tidymodels (we'll see the normal convention in just a minute with workflows) but its a good sanity check for confirming that recipes are doing what you expect.

For that, you'll need two more verbs: prep() and bake()

prep(): estimates the required parameters from a training set that can be later applied to other data sets.

bake(): takes a prepped recipe and applies the operations to any data set.

Now let's prep and bake our recipes to confirm that under the hood, the predictor columns will be first encoded before a model is fit.

```{r}
# Prep the recipe
pumpkins_prep <- prep(pumpkins_recipe)

# Bake the recipe to extract a preprocessed new_pumpkins data
baked_pumpkins <- bake(pumpkins_prep, new_data = NULL)

# Print out the baked data set
baked_pumpkins %>% 
  slice_head(n = 10)
```

The processed data baked_pumpkins has all its predictors encoded confirming that indeed the preprocessing steps defined as our recipe will work as expected. This makes it harder for you to read but more intelligible for tidymodels. Take a look at how the observations have been mapped to numbers.

*Question 6:* From looking at the baked_pumpkins tibble, how many total cities are represented in the data set?

**10 cities are represented in the dataset.**

baked_pumpkins is a data frame that we can perform computations on. For instance, let's try to find a good correlation between two variables to potentially build a good predictive model. We'll use the function cor() to do this.

```{r}
# Find the correlation between the package and the price
cor(baked_pumpkins$package, baked_pumpkins$price)
```

*Question 7:* Calculate the correlation between pumpkin price and two other variables in the data set

```{r,echo=TRUE}
cor(baked_pumpkins$variety, baked_pumpkins$price)

cor(baked_pumpkins$day, baked_pumpkins$price)
```

*Question 8:* Which of these three variables is most highly correlated with price? Why might this be?

**The "variety" variable is most highly correlated with price, because it has a correlation of -.86 which is closer to -1 indicating a strong negative correlation compared the variable "day" which has a correlation closer to 0. The different varieties impact the price of the pumpkins.**

Now let's visualize a correlation matrix of all the columns using the corrplot package.

```{r}
# Load the corrplot package
library(corrplot)

# Obtain correlation matrix
corr_mat <- cor(baked_pumpkins %>% 

# Drop columns that are not really informative
select(-c(low_price, high_price)))

# Make a correlation plot between the variables
corrplot(corr_mat, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")

```

# Build a linear regression model

Now that we have build a recipe, and actually confirmed that the data will be pre-processed appropriately, let's now build a regression model to answer the question: What price can I expect of a given pumpkin package?

# Train a linear regression model using the training set As you may have already figured out, the column price is the outcome variable while the package column is the predictor variable.

To do this, we'll first split the data. Data splitting is a key part of the machine learning process. For now we'll do a 80/2o split, where 80% of the data goes into training and 20% into the test set. Then we'll define a recipe that will encode the predictor column into a set of integers, then build a model specification. We won't prep and bake our recipe since we already know it will preprocess the data as expected.

```{r}
set.seed(123)
# Split the data into training and test sets
pumpkins_split <- new_pumpkins %>% 
  initial_split(prop = 0.8)


# Extract training and test data
pumpkins_train <- training(pumpkins_split)
pumpkins_test <- testing(pumpkins_split)


# Create a recipe for preprocessing the data
lm_pumpkins_recipe <- recipe(price ~ package, data = pumpkins_train) %>% 
  step_integer(all_predictors(), zero_based = TRUE)


# Create a linear model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```

Now that we have a recipe and a model specification, we need to find a way of bundling them together into an object that will first preprocess the data (prep+bake behind the scenes), fit the model on the preprocessed data and also allow for potential post-processing activities.

So let's bundle everything up into a workflow. A workflow is a container object that aggregates information required to fit and predict from a model.

```{r}
# Hold modelling components in a workflow
lm_wf <- workflow() %>% 
  add_recipe(lm_pumpkins_recipe) %>% 
  add_model(lm_spec)

# Print out the workflow
lm_wf
```

A workflow can be fit/trained in much the same way a model can.

```{r}
# Train the model
lm_wf_fit <- lm_wf %>% 
  fit(data = pumpkins_train)

# Print the model coefficients learned 
lm_wf_fit
```

From the model output, we can see the coefficients learned during training. They represent the coefficients of the line of best fit that gives us the lowest overall error between the actual and predicted variable.

Evaluate model performance using the test set. It's time to see how the model performed! How do we do this?

Now that we've trained the model, we can use it to make predictions for the test_set using parsnip::predict(). Then we can compare these predictions to the actual label values to evaluate how well (or not!) the model is working.

Let's start with making predictions for the test set then bind the columns to the test set.

```{r prediction_test}
# Make predictions for the test set
predictions <- lm_wf_fit %>% 
  predict(new_data = pumpkins_test)


# Bind predictions to the test set
lm_results <- pumpkins_test %>% 
  select(c(package, price)) %>% 
  bind_cols(predictions)


# Print the first ten rows of the tibble
lm_results %>% 
  slice_head(n = 10)
```

OK, you have just trained a model and used it to make predictions! Let's evaluate the model's performance.

In Tidymodels, we do this using yardstick::metrics(). For linear regression, let's focus on the following metrics:

Root Mean Square Error (RMSE): The square root of the MSE. This yields an absolute metric in the same unit as the label (in this case, the price of a pumpkin). The smaller the value, the better the model (in a simplistic sense, it represents the average price by which the predictions are wrong)

Coefficient of Determination (usually known as R-squared or R2): A relative metric in which the higher the value, the better the fit of the model. In essence, this metric represents how much of the variance between predicted and actual label values the model is able to explain.

```{r evaluate_lr}
# Evaluate performance of linear regression
metrics(data = lm_results,
        truth = price,
        estimate = .pred)
```

OK, so that is the model performance. Let's see if we can get a better indication by visualizing a scatter plot of the package and price then use the predictions made to overlay a line of best fit.

This means we'll have to prep and bake the test set in order to encode the package column then bind this to the predictions made by our model.

```{r encode_package}
# Encode package column
package_encode <- lm_pumpkins_recipe %>% 
  prep() %>% 
  bake(new_data = pumpkins_test) %>% 
  select(package)


# Bind encoded package column to the results
 plot_results <- lm_results %>%
 bind_cols(package_encode %>%
               rename(package_integer = package)) %>%
  relocate(package_integer, .after = package)


# Print new results data frame
plot_results %>%
  slice_head(n = 5)


# Make a scatter plot
plot_results %>%
  ggplot(mapping = aes(x = package_integer, y = price)) +
   geom_point(size = 1.6) +
   # Overlay a line of best fit
   geom_line(aes(y = .pred), color = "orange", size = 1.2) +
   xlab("package")
```

**Today we will be continuing the pumpkin case study from last week. We will be using the data that you cleaned and split last time (pumpkins_train) and will be comparing our results today to those you have already obtained, so open and run your Lab 1 .Rmd as a first step so those objects are available in your Environment (unless you created an R Project last time, in which case, kudos to you!).** 

Once you have done that, we'll start today's lab by specifying a recipe for a polynomial model.  First we specify a recipe that identifies our variables and data, converts package to a numerical form, and then add a polynomial effect with step_poly()

```{r}
# Specify a recipe
poly_pumpkins_recipe <-
  recipe(price ~ package, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE) %>% 
  step_poly(all_predictors(), degree = 2)
```

How did that work? Choose another value for degree if you need to. Later we will learn about model tuning that will let us do things like find the optimal value for degree.  For now, we'd like to have a flexible model, so find the highest value for degree that is consistent with our data.

Polynomial regression is still linear regression, so our model specification looks similar to before.

```{r}
# Create a model specification called poly_spec
poly_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```
Question 1: Now take the recipe and model specification that just created and bundle them into a workflow called poly_df.

```{r}
# Bundle recipe and model spec into a workflow
poly_wf <- workflow() %>% 
  add_recipe(poly_pumpkins_recipe) %>% 
  add_model(poly_spec)
```

Question 2: fit a model to the pumpkins_train data using your workflow and assign it to poly_wf_fit
```{r}
# Create a model
poly_wf_fit <- poly_wf %>% 
  fit(data = pumpkins_train)
```

```{r}
# Print learned model coefficients
poly_wf_fit
```

```{r}
# Make price predictions on test data
poly_results <- poly_wf_fit %>% predict(new_data = pumpkins_test) %>% 
  bind_cols(pumpkins_test %>% select(c(package, price))) %>% 
  relocate(.pred, .after = last_col())

# Print the results
poly_results %>% 
  slice_head(n = 10)
```

Now let's evaluate how the model performed on the test_set using yardstick::metrics().
```{r}
metrics(data = poly_results, truth = price, estimate = .pred)
```
Question 3: How do the performance metrics differ between the linear model from last week and the polynomial model we fit today?  Which model performs better on predicting the price of different packages of pumpkins?

Let's visualize our model results.  First prep the results by binding the encoded package variable to them.
```{r}
# Bind encoded package column to the results
poly_results <- poly_results %>% 
  bind_cols(package_encode %>% 
              rename(package_integer = package)) %>% 
  relocate(package_integer, .after = package)


# Print new results data frame
poly_results %>% 
  slice_head(n = 5)
```

OK, now let's take a look! 

Question 4: Create a scatter plot that takes the poly_results and plots package vs. price.  Then draw a line showing our model's predicted values (.pred). Hint: you'll need separate geoms for the data points and the prediction line.
```{r}
# Make a scatter plot
poly_results %>% 
  ggplot(mapping = aes(x = package_integer, y = price)) +
   geom_point(size = 1.6) +
   # Overlay a line of best fit
   geom_line(aes(y = .pred), color = "orange", size = 1.2) +
   xlab("package")

```

You can see that a curved line fits your data much better.

Question 5: Now make a smoother line by using geom_smooth instead of geom_line and passing it a polynomial formula like this:
geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "midnightblue", size = 1.2, se = FALSE)

```{r}
# Make a smoother scatter plot 
poly_results %>% 
  ggplot(mapping = aes(x = package_integer, y = price)) +
   geom_point(size = 1.6) +
   # Overlay a geom_smooth line
   xlab("package") + geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "midnightblue", size = 1.2, se = FALSE)
```

OK, now it's your turn to go through the process one more time.
 
Additional assignment components :
6. Choose  a new predictor variable (anything not involving package type) in this dataset.

```{r}
# Names of all predictor variables
names(baked_pumpkins)
```

**My new predictor variable is variety.**

7. Determine its correlation with the outcome variable (price).  (Remember we calculated a correlation matrix last week)

```{r}
# Calculate the correlation between pumpkin price and variety
cor(baked_pumpkins$variety, baked_pumpkins$price)

# Obtain correlation matrix and drop low_price and high
corr_matrix <- cor(baked_pumpkins %>% 
# Drop columns that are not really informative
  select(-c(low_price, high_price, month, day, date)))

# Make a correlation plot between the variables
corrplot(corr_matrix, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")
```

8. Create and test a model for your new predictor:
  - Create a recipe
```{r}
# Specify a recipe
pumpkins_recipe_variety <-
  recipe(price ~ variety, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE) 
#%>% step_poly(all_predictors(), degree = 3)
```
  
  - Build a model specification (linear or polynomial)
```{r}
# Create a model specification
poly_spec_var <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```
  
  - Bundle the recipe and model specification into a workflow
```{r}
# Bundle recipe and model spec into a workflow
poly_wf_var <- workflow() %>% 
  add_recipe(pumpkins_recipe_variety) %>% 
  add_model(poly_spec_var)
```
  
  - Create a model by fitting the workflow
```{r}
# Create a model
poly_wf_fit_var <- poly_wf %>% 
  fit(data = pumpkins_train)
```

  - Evaluate model performance on the test data
```{r}
# Make price predictions 
poly_results_var <- pumpkins_test %>% 
  select(c(variety, price)) %>% 
  bind_cols(predictions)

# Print the results
poly_results_var %>% 
  slice_head(n = 10)

# Evaluate model performance
metrics(data = poly_results_var, truth = price, estimate = .pred)
```
  
  - Create a visualization of model performance
```{r}
# Encode package column
encode_var <- pumpkins_recipe_variety %>%
  prep() %>%
  bake(new_data = pumpkins_test) %>%
  select(variety)

# Bind encoded package column to the results
 plot_results_var <- poly_results_var %>%
 bind_cols(encode_var %>%
               rename(variety_integer = variety)) %>%
  relocate(variety_integer, .after = variety)

# Print new results data frame
plot_results %>%
  slice_head(n = 5)

# Make a scatter plot
plot_results_var %>%
  ggplot(mapping = aes(x = variety_integer, y = price)) +
   geom_point(size = 1.6) +
   # Overlay a line of best fit
   geom_line(aes(y = .pred), color = "orange", size = 1.2) +
   xlab("variety")


# Graph with geom_smooth
plot_results_var %>%
  ggplot(mapping = aes(x = variety_integer, y = price)) +
   geom_point(size = 1.6) +
   # Overlay a geom_smooth line
   xlab("variety") + geom_smooth(method = lm, formula = y ~ poly(x, degree = 2), color = "midnightblue", size = 1.2, se = FALSE)
```

  
Lab 2 due 1/24 at 11:59 PM